{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmzsMkKYqNoa"
      },
      "outputs": [],
      "source": [
        "# âœ… Colab Cell 1: Install Dependencies and Configure Ngrok\n",
        "\n",
        "# Install required Python packages\n",
        "!pip install flask flask-cors unsloth gtts --quiet\n",
        "\n",
        "# Fix a debugpy_repr issue that can occur in Colab with some libraries\n",
        "import google.colab._debugpy_repr as dbg\n",
        "dbg.get_shape = lambda obj: None\n",
        "\n",
        "# Download and unzip Ngrok executable\n",
        "!wget -q -O ngrok.zip https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -o ngrok.zip > /dev/null\n",
        "!chmod +x ngrok\n",
        "\n",
        "# --- IMPORTANT: Replace YOUR_NGROK_AUTHTOKEN_HERE with your actual Ngrok authtoken ---\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "NGROK_AUTH_TOKEN = 'YOUR_TOKEN' # <--- REPLACE THIS LINE\n",
        "!./ngrok config add-authtoken {NGROK_AUTH_TOKEN}\n",
        "\n",
        "print(\"Dependencies installed and Ngrok configured.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "da08yFe5qstu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify, send_file\n",
        "from flask_cors import CORS\n",
        "import threading, time, requests, torch\n",
        "import re\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "from gtts import gTTS\n",
        "from io import BytesIO\n",
        "\n",
        "print(\"Google Drive mounted and libraries imported.\")"
      ],
      "metadata": {
        "id": "cqsCoDJnqWVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Colab Cell 3: Load Your Fine-Tuned AI Model\n",
        "\n",
        "# IMPORTANT: Adjust this path to where your fine-tuned model is saved in Google Drive\n",
        "model_path = \"\"\n",
        "\n",
        "print(f\"Loading model from: {model_path}\")\n",
        "try:\n",
        "    base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\",\n",
        "        max_seq_length = 2048,\n",
        "        dtype = torch.float16, # Use torch.bfloat16 if your GPU supports it (e.g., A100, H100)\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(base_model, model_path)\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Please ensure your model path is correct and Google Drive is mounted.\")\n",
        "    # Exit or handle error appropriately if model fails to load\n",
        "    exit() # This will stop the Colab execution if the model doesn't load"
      ],
      "metadata": {
        "id": "7mXVKpqfqX6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Colab Cell 4: Initialize Flask App and Define Chat Endpoint\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Initial system prompt for the main therapy bot\n",
        "chat_history = [{\n",
        "    \"role\": \"system\",\n",
        "    \"content\": (\n",
        "        \"\"\"You are 'Aura', a supportive and empathetic AI companion. Your purpose is to provide a safe, non-judgmental space for users to discuss their feelings and to offer practical, evidence-based coping mechanisms and techniques (like mindfulness, breathing exercises, or grounding).\n",
        "\n",
        "Your core directives are:\n",
        "1.  **Provide Support & Coping Mechanisms:** Focus on validating feelings and suggesting helpful strategies.\n",
        "2.  **Do Not Diagnose:** You are not a medical professional. Never diagnose conditions. If asked, gently state this limitation and suggest consulting a qualified professional.\n",
        "3.  **Stay On Topic:** Your expertise is limited to mental and emotional support. If a user asks about unrelated topics (e.g., coding, homework, general trivia), politely decline and state your purpose is to be a supportive listener.\n",
        "4.  **CRITICAL SAFETY PROTOCOL:** If a user expresses clear intent, a plan, or is actively attempting self-harm or harm to others, you must immediately stop the conversation and respond with ONLY the following text: \"It sounds like you are in crisis, and your safety is the most important thing. It's vital to speak with someone who can provide immediate help. Please contact a suicide prevention hotline. In India, you can reach Vandrevala Foundation at 9999666555 or Aasra at +91-9820466726. Please reach out to them now.\"\n",
        "\"\"\"\n",
        "    )\n",
        "}]\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_input = request.json.get('message', '').strip()\n",
        "    if not user_input:\n",
        "        return jsonify({'reply': 'âš ï¸ Please enter a message'}), 400\n",
        "\n",
        "    response_type = request.json.get('response_type', 'text').lower()\n",
        "\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        chat_history,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    input_len = inputs.input_ids.shape[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_tokens = output[0][input_len:]\n",
        "    reply = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "    if response_type == 'audio':\n",
        "        try:\n",
        "            tts = gTTS(text=reply, lang='en', slow=False)\n",
        "            audio_stream = BytesIO()\n",
        "            tts.write_to_fp(audio_stream)\n",
        "            audio_stream.seek(0)\n",
        "            return send_file(audio_stream, mimetype=\"audio/mpeg\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating TTS: {e}\")\n",
        "            return jsonify({\"error\": f\"Could not generate audio: {e}\", \"text_response\": reply}), 500\n",
        "    else:\n",
        "        return jsonify({'reply': reply})\n",
        "\n",
        "print(\"Flask app defined.\")"
      ],
      "metadata": {
        "id": "mgwv8IUCqZ2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Colab Cell 5: Launch Flask in Background and Start Ngrok Tunnel\n",
        "\n",
        "def run_flask():\n",
        "    print(\"Starting Flask app on port 5000...\")\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
        "\n",
        "# Start Flask in a separate thread so Ngrok can run in the main thread\n",
        "flask_thread = threading.Thread(target=run_flask)\n",
        "flask_thread.daemon = True # Allows the main program to exit even if the thread is running\n",
        "flask_thread.start()\n",
        "print(\"Flask app thread started.\")\n",
        "\n",
        "# Give Flask a moment to start up\n",
        "time.sleep(5)\n",
        "print(\"Starting Ngrok tunnel...\")\n",
        "# This runs ngrok in the background. get_ipython().system_raw is for Colab.\n",
        "get_ipython().system_raw('./ngrok http 5000 &')\n",
        "\n",
        "# Function to retrieve public Ngrok URL\n",
        "def get_ngrok_url(retries=30, delay=2): # Increased retries to be more robust\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            r = requests.get('http://localhost:4040/api/tunnels')\n",
        "            tunnels = r.json()['tunnels']\n",
        "            if tunnels:\n",
        "                # Find the HTTPS public URL\n",
        "                public_url = next((t['public_url'] for t in tunnels if t['proto'] == 'https'), None)\n",
        "                if public_url:\n",
        "                    return public_url\n",
        "            print(f\"Ngrok URL not yet available or not HTTPS, retrying... ({i+1}/{retries})\")\n",
        "            time.sleep(delay)\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(f\"Could not connect to Ngrok API, retrying... ({i+1}/{retries})\")\n",
        "            time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while getting Ngrok URL: {e}, retrying... ({i+1}/{retries})\")\n",
        "            time.sleep(delay)\n",
        "    return None\n",
        "\n",
        "public_url = get_ngrok_url()\n",
        "if public_url:\n",
        "    print(f\"\\nðŸ§  AI Therapist is live! Access it at: {public_url}/chat\")\n",
        "    print(\"\\nTo chat with your bot and get a TEXT response, use `curl` or a simple Python script:\")\n",
        "    print(f\"Example `curl` command (TEXT):\\ncurl -X POST -H \\\"Content-Type: application/json\\\" -d '{{\\\"message\\\": \\\"I\\\\'m feeling really anxious today.\\\", \\\"response_type\\\": \\\"text\\\"}}' {public_url}/chat\")\n",
        "    print(f\"\\nExample Python request (TEXT):\\nimport requests\\nurl = \\\"{public_url}/chat\\\"\\nheaders = {{'Content-Type': 'application/json'}}\\ndata = {{'message': 'I\\\\'m having a tough time with stress.', 'response_type': 'text'}}\\nresponse = requests.post(url, headers=headers, json=data)\\nprint(response.json())\")\n",
        "    print(\"\\nTo chat with your bot and get an AUDIO response, use `curl` or a simple Python script (saves to file):\")\n",
        "    print(f\"Example `curl` command (AUDIO):\\ncurl -X POST -H \\\"Content-Type: application/json\\\" -d '{{\\\"message\\\": \\\"I\\\\'m feeling really anxious today.\\\", \\\"response_type\\\": \\\"audio\\\"}}' {public_url}/chat --output bot_response.mp3\")\n",
        "    print(f\"\\nExample Python request (AUDIO, saves to file):\\nimport requests\\nurl = \\\"{public_url}/chat\\\"\\nheaders = {{'Content-Type': 'application/json'}}\\ndata = {{'message': 'I\\\\'m having a tough time with stress.', 'response_type': 'audio'}}\\nresponse = requests.post(url, headers=headers, json=data)\\nif response.headers.get('Content-Type') == 'audio/mpeg':\\n    with open('bot_audio_response.mp3', 'wb') as f:\\n        f.write(response.content)\\n    print('Audio response saved to bot_audio_response.mp3')\\nelse:\\n    print('Non-audio response received:', response.text)\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Ngrok failed to start or retrieve public URL.\")\n",
        "    print(\"Please check your Ngrok authtoken and ensure the Flask app started correctly.\")\n",
        "\n",
        "print(\"\\nServer setup complete. Keep this cell running to maintain the server and Ngrok tunnel.\")"
      ],
      "metadata": {
        "id": "hfzpiY1xqc8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Colab Cell 6: Keep Session Alive and Clean Up\n",
        "\n",
        "# This loop keeps the Colab notebook session alive and the server running.\n",
        "# Interrupt this cell (e.g., click the stop button in Colab) to stop the server and Ngrok.\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nServer stopped. Cleaning up ngrok tunnel.\")\n",
        "    # Attempt to kill ngrok process gracefully\n",
        "    !pkill -f ngrok\n",
        "    !sudo lsof -i :5000\n",
        "    print(\"Ngrok tunnel terminated.\")"
      ],
      "metadata": {
        "id": "RS_kq81Bqe7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!fuser -k 5000/tcp\n"
      ],
      "metadata": {
        "id": "3rSa2w802m0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}